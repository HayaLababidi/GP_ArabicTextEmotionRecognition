{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Recognition Arabic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNh0MX46f8JtrflPxGLEbUP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayaLababidi/GP_ArabicTextEmotionRecognition/blob/master/Emotion_Recognition_Arabic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwZKLhO-eNwj"
      },
      "source": [
        "# installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cHSzaeuCK78",
        "outputId": "9ac209ed-4b52-4259-a628-87081c1c1170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install gensim\n",
        "!pip install sklearn\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install --upgrade google-api-python-client"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.63)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.63 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.63)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Collecting google-api-python-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/ee/aa94b1af0d5d426a31b89d12e069c64c071e15efdafad70023858a4d52fe/google_api_python_client-1.12.2-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.15.0)\n",
            "Collecting google-api-core<2dev,>=1.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/45/a5707ce2cbd6681c09e69a0ce10d9bebe98e0231c458a4dc652670f3584f/google_api_core-1.22.2-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.17.4)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.16.0->google-api-python-client) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (1.24.3)\n",
            "\u001b[31mERROR: google-api-core 1.22.2 has requirement google-auth<2.0dev,>=1.21.1, but you'll have google-auth 1.17.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-api-core, google-api-python-client\n",
            "  Found existing installation: google-api-core 1.16.0\n",
            "    Uninstalling google-api-core-1.16.0:\n",
            "      Successfully uninstalled google-api-core-1.16.0\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed google-api-core-1.22.2 google-api-python-client-1.12.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgHx4XTjEidm",
        "outputId": "a4ea27f9-3474-46d9-c4e1-d36495a9780f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!git clone https://github.com/HayaLababidi/GP_ArabicTextEmotionRecognition.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GP_ArabicTextEmotionRecognition'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 409 (delta 0), reused 0 (delta 0), pack-reused 406\u001b[K\n",
            "Receiving objects: 100% (409/409), 7.14 MiB | 2.64 MiB/s, done.\n",
            "Resolving deltas: 100% (183/183), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuRyawsfFrbM",
        "outputId": "812127aa-a1b8-41da-e402-c0b2edda2acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "!wget https://ia902903.us.archive.org/26/items/full_grams_cbow_300_twitter/full_grams_sg_100_twitter.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-27 15:03:11--  https://ia902903.us.archive.org/26/items/full_grams_cbow_300_twitter/full_grams_sg_100_twitter.zip\n",
            "Resolving ia902903.us.archive.org (ia902903.us.archive.org)... 207.241.233.43\n",
            "Connecting to ia902903.us.archive.org (ia902903.us.archive.org)|207.241.233.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://archive.org/download/full_grams_cbow_300_twitter/full_grams_sg_100_twitter.zip [following]\n",
            "--2020-09-27 15:03:12--  https://archive.org/download/full_grams_cbow_300_twitter/full_grams_sg_100_twitter.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia802803.us.archive.org/15/items/full_grams_cbow_300_twitter/full_grams_sg_100_twitter.zip [following]\n",
            "--2020-09-27 15:03:12--  https://ia802803.us.archive.org/15/items/full_grams_cbow_300_twitter/full_grams_sg_100_twitter.zip\n",
            "Resolving ia802803.us.archive.org (ia802803.us.archive.org)... 207.241.232.113\n",
            "Connecting to ia802803.us.archive.org (ia802803.us.archive.org)|207.241.232.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1132025304 (1.1G) [application/zip]\n",
            "Saving to: ‘full_grams_sg_100_twitter.zip’\n",
            "\n",
            "full_grams_sg_100_t 100%[===================>]   1.05G  1.27MB/s    in 20m 49s \n",
            "\n",
            "2020-09-27 15:24:01 (885 KB/s) - ‘full_grams_sg_100_twitter.zip’ saved [1132025304/1132025304]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWIESSWUFwgu",
        "outputId": "364bc745-44eb-4684-81ef-b3450428cf98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "%cd /content/GP_ArabicTextEmotionRecognition/GP_PythonFiles/models\n",
        "!unzip /content/full_grams_sg_100_twitter.zip\n",
        "%cd /content/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/full_grams_sg_100_twitter.zip\n",
            "  inflating: full_grams_sg_100_twitter.mdl  \n",
            "  inflating: full_grams_sg_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_sg_100_twitter.mdl.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiJFJwrFebtW"
      },
      "source": [
        "# model and demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlaxcRigCXML",
        "cellView": "form"
      },
      "source": [
        "#@title model\n",
        "import keras\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import itertools\n",
        "import gensim\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\n",
        "import string\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from googleapiclient.discovery import build\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "class Data_operations:\n",
        "    my_api_key = \"AIzaSyCUKEOsT6ecC3ods862vgsVOawWyii0NDQ\"\n",
        "    my_cse_id = \"007967891901694126580:i3iq-cjlldq\"\n",
        "\n",
        "    def __init__(self):  # , my_api_key, my_cse_id):\n",
        "        self._dictionary = None\n",
        "        self._test_size = 0.1\n",
        "        self.out_of_vocab = 0\n",
        "        self.in_vocab = 0\n",
        "        self._aravec_model_name = r\"/content/GP_ArabicTextEmotionRecognition/GP_PythonFiles/models/full_grams_sg_100_twitter\"\n",
        "        self.arabic_punctuations = '''`÷×؛<>_()*&^%][،/:\"؟.,'{}~¦+|!”…“–»«•'''\n",
        "        self.english_punctuations = string.punctuation\n",
        "        self.english_numbers = \"0123456789\"\n",
        "        self.punctuations_list = self.arabic_punctuations + self.english_punctuations + self.english_numbers\n",
        "        self.t_model = None\n",
        "        self._number_of_inputs = 140\n",
        "        self._vector_size = 100\n",
        "        self.arabic_diacritics = re.compile(\"\"\"\n",
        "                                        ـ    | # empty line in between letters (longation) \n",
        "                                         ّ    | # Tashdid\n",
        "                                         َ    | # Fatha\n",
        "                                         ً    | # Tanwin Fath\n",
        "                                         ُ    | # Damma\n",
        "                                         ٌ    | # Tanwin Damm\n",
        "                                         ِ    | # Kasra\n",
        "                                         ٍ    | # Tanwin Kasr\n",
        "                                         ْ    | # Sukun\n",
        "                                     \"\"\", re.VERBOSE)\n",
        "        self.google_api_key = \"AIzaSyCUKEOsT6ecC3ods862vgsVOawWyii0NDQ\"\n",
        "        self.cse_id = \"007967891901694126580:i3iq-cjlldq\"\n",
        "\n",
        "    # mode 0 functions\n",
        "    def google_search(self, search_term):\n",
        "        service = build(\"customsearch\", \"v1\", developerKey=self.google_api_key)\n",
        "        res = service.cse().list(q=search_term, cx=self.cse_id).execute()\n",
        "        return res['spelling']['correctedQuery']\n",
        "\n",
        "    def normalize_arabic(self, text):\n",
        "        text = re.sub(\"إ\", \"ا\", text)\n",
        "        text = re.sub(\"أ\", \"ا\", text)\n",
        "        text = re.sub(\"آ\", \"ا\", text)\n",
        "        text = re.sub(\"ا\", \"ا\", text)\n",
        "        # text = re.sub(\"ى\", \"ي\", text)\n",
        "        # text = re.sub(\"ؤ\", \"ء\", text)\n",
        "        # text = re.sub(\"ئ\", \"ء\", text)\n",
        "        # text = re.sub(\"ة\", \"ه\", text)\n",
        "        text = re.sub(\"گ\", \"ك\", text)\n",
        "        return text\n",
        "\n",
        "    def remove_diacritics(self, text):\n",
        "        text = re.sub(self.arabic_diacritics, '', text)\n",
        "        return text\n",
        "\n",
        "    def remove_punctuations(self, text):\n",
        "        for c in self.punctuations_list:\n",
        "            text = text.replace(c, \" \")\n",
        "        return text\n",
        "\n",
        "    def remove_repeating_char(self, text):\n",
        "        return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "    def tokens_remove_stopwords(self, text):\n",
        "        text = text.split()\n",
        "        result = list()\n",
        "        ch = 0\n",
        "\n",
        "        arabic_stop_words = [\"من\", \"فى\", \"الي\", \"علي\", \"عن\", \"حتي\", \"مذ\", \"منذ\", \"و\", \"الا\", \"او\", \"ام\", \"ثم\", \"بل\",\n",
        "                             \"لكن\",\n",
        "                             \"كل\", \"متى\", \"يوم\"]\n",
        "\n",
        "        for word in text:\n",
        "            for stop_word in arabic_stop_words:\n",
        "                if word == stop_word:\n",
        "                    ch = 1\n",
        "                    break\n",
        "\n",
        "            if ch != 1:\n",
        "                result.append(word)\n",
        "\n",
        "            ch = 0\n",
        "\n",
        "        return result\n",
        "\n",
        "    def rooting(self, text):\n",
        "        result = list()\n",
        "        for word in text:\n",
        "            stemmer = ISRIStemmer()\n",
        "            result.append(stemmer.stem(word))\n",
        "        return result\n",
        "\n",
        "    def remove_english(self, tokens):\n",
        "        filtered_tokens = list()\n",
        "        for word in tokens:\n",
        "            if (not re.match(r'[a-zA-Z]+', word, re.I)) and word != '':\n",
        "                filtered_tokens.append(word)\n",
        "        return filtered_tokens\n",
        "\n",
        "    def preprocess_doc(self, text):\n",
        "        text = str(text)\n",
        "        text = self.remove_diacritics(text)\n",
        "        text = self.remove_punctuations(text)\n",
        "        text = self.normalize_arabic(text)\n",
        "        text = self.remove_repeating_char(text)\n",
        "        tokens = re.split(\" \", text)\n",
        "        tokens = self.remove_english(tokens)\n",
        "        return tokens\n",
        "\n",
        "    def embed_doc_word(self, text):\n",
        "        if self.t_model is None:\n",
        "            self.t_model = gensim.models.Word2Vec.load(self._aravec_model_name + '.mdl')\n",
        "\n",
        "        preprocessed_text = self.preprocess_doc(text)\n",
        "        # print(preprocessed_text)\n",
        "\n",
        "        embedded_vectors = np.zeros(\n",
        "            shape=(\n",
        "            self._number_of_inputs, self._vector_size))  # np array of arrays (array of 100/300 float number per word)\n",
        "        embedded_vectors_index = 0\n",
        "        for i in range(len(preprocessed_text)):\n",
        "            if embedded_vectors_index > self._number_of_inputs:\n",
        "                break\n",
        "            try:\n",
        "                embedded_vectors[embedded_vectors_index] = self.t_model.wv[preprocessed_text[i]]\n",
        "                embedded_vectors_index = embedded_vectors_index + 1\n",
        "            except:\n",
        "                try:\n",
        "                    result = self.rooting([preprocessed_text[i]])[0]\n",
        "                    embedded_vectors[embedded_vectors_index] = self.t_model.wv[result]\n",
        "                    embedded_vectors_index = embedded_vectors_index + 1\n",
        "                except:\n",
        "                    try:\n",
        "                        # print(self,\"in google search \" + preprocessed_text[i])\n",
        "                        search_output = self.google_search(preprocessed_text[i])\n",
        "                        # print(\"search_output \" + search_output)\n",
        "                        tokens = re.split(\" \", search_output)\n",
        "                        for j in range(len(tokens)):\n",
        "                            try:\n",
        "                                embedded_vectors[embedded_vectors_index] = self.t_model.wv[tokens[j]]\n",
        "                                embedded_vectors_index = embedded_vectors_index + 1\n",
        "                                #print(\"added \" + tokens[j])\n",
        "                            except:\n",
        "                                pass\n",
        "                                #print(tokens[j] + \" Sub word cant be embedded\")\n",
        "                    except:\n",
        "                        # print(preprocessed_text[i] + \"word cant be embedded\") #currently emojis can't be embedded and for any extreme case (skip wrongly written words)\n",
        "                        self.out_of_vocab = self.out_of_vocab + 1\n",
        "        self.in_vocab = self.in_vocab + embedded_vectors_index\n",
        "        return embedded_vectors, self.out_of_vocab, self.in_vocab\n",
        "\n",
        "    def embed_dataset_word(self, X_train, X_test):\n",
        "        eX_train = np.zeros(shape=(len(X_train), self._number_of_inputs, self._vector_size),\n",
        "                            dtype=np.float16)  # number of tweets*max number of words per tweet*vector size per word\n",
        "        eX_test = np.zeros(shape=(len(X_test), self._number_of_inputs, self._vector_size), dtype=np.float16)\n",
        "\n",
        "        self.out_of_vocab = 0\n",
        "        self.out_of_vocab = 0\n",
        "        for i in range(len(X_train)):\n",
        "            eX_train[i], self.out_of_vocab, self.out_of_vocab = self.embed_doc_word(X_train[i])\n",
        "\n",
        "        for i in range(len(X_test)):\n",
        "            eX_test[i], self.out_of_vocab, self.out_of_vocab = self.embed_doc_word(X_test[i])\n",
        "        # print(\"out emo\", self.out_of_vocab)\n",
        "        # print(\"in emo\", self.in_vocab)\n",
        "        return eX_train, eX_test\n",
        "\n",
        "    # mode 2 functions\n",
        "    def get_dictonary(self, dataset):\n",
        "        uniques = ''\n",
        "        row = ''\n",
        "        for text in dataset:\n",
        "            # row = ''\n",
        "            try:\n",
        "                row = row + ''.join(set(text[0]))\n",
        "            except:\n",
        "                pass\n",
        "            # uniques = uniques.join(set(row)) #append(row)\n",
        "\n",
        "            # print(\"row:\", row)\n",
        "        uniques = uniques.join(set(row))\n",
        "        #print(\"uniques:\", uniques)\n",
        "        # uniques = (set(uniques))\n",
        "        length = len(uniques)\n",
        "        #\n",
        "        #print(length)\n",
        "        indexes = list(range(length))\n",
        "\n",
        "        di = dict(zip(uniques, indexes))\n",
        "        return di\n",
        "\n",
        "    def convert_to_int_doc(self, text, dictionary):\n",
        "\n",
        "        row_length = 288\n",
        "        padding = len(dictionary)\n",
        "        row = []\n",
        "        try:\n",
        "            for char in text[0]:\n",
        "                number = dictionary[char]\n",
        "                row.append(number)\n",
        "            length = len(row)\n",
        "            for i in range(length, row_length):\n",
        "                row.append(padding)\n",
        "        except:\n",
        "            length = len(row)\n",
        "            for i in range(length, row_length):\n",
        "                row.append(padding)\n",
        "        return np.array(row)\n",
        "\n",
        "    def one_hot_encode_doc(self, dictionary, text):\n",
        "        onehot_text = []\n",
        "        row_length = 288\n",
        "        padding = np.zeros(len(dictionary))\n",
        "        if text[0] is not None:\n",
        "            try:\n",
        "                for character in text[0]:\n",
        "                    vector = np.zeros(len(dictionary))\n",
        "                    index = dictionary[character]\n",
        "                    vector[index] = 1\n",
        "                    onehot_text.append(vector)\n",
        "            except :\n",
        "                pass\n",
        "        length = len(onehot_text)\n",
        "        for i in range(length, row_length):\n",
        "            onehot_text.append(padding)\n",
        "        return onehot_text\n",
        "\n",
        "    def one_hot_encode_dataset(self, dictionary,dataset):\n",
        "        data_array2d = []\n",
        "        for row in dataset:\n",
        "            temp = []\n",
        "            temp = self.one_hot_encode_doc(dictionary, row)\n",
        "            data_array2d.append(temp)\n",
        "\n",
        "        return data_array2d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def one_hot_encode(dataset):\n",
        "        max_length = 0\n",
        "        data_Array2d = []\n",
        "\n",
        "        for i in range(len(dataset)):\n",
        "            try:\n",
        "                chars = list(dataset[i][0])\n",
        "                cur_length = len(dataset[i][0])\n",
        "                if cur_length > max_length:\n",
        "                    max_length = cur_length\n",
        "            except:\n",
        "                #print(\"skipped row{}\".format(i))\n",
        "                continue\n",
        "\n",
        "            length = len(chars)\n",
        "\n",
        "            for indexRow in range(length):\n",
        "                temp = []\n",
        "                temp.append(chars[indexRow])\n",
        "                data_Array2d.append(temp)\n",
        "        #print(\"max_length of sentences\", max_length)\n",
        "\n",
        "        hot = OneHotEncoder(handle_unknown='ignore')\n",
        "        # print(\"dataset array length\",len(data_Array2d))\n",
        "        hot.fit(data_Array2d)\n",
        "        # print(\"categorys\",hot.categories)\n",
        "\n",
        "        encoded_dataset = np.zeros(shape=(len(dataset), max_length, 155))  # list of encoded rows\n",
        "        encoded_row = np.zeros(shape=(max_length, 155))  # vectors for each char in dataset row\n",
        "\n",
        "        # for each row get 2darray of char\n",
        "        # encode each char ,append encoded_row\n",
        "        # append encoded dataset\n",
        "\n",
        "        for i in range(len(dataset)):\n",
        "            row = dataset[i]\n",
        "            try:\n",
        "                chars = list(row[0])\n",
        "            except:\n",
        "                pass\n",
        "                #print(row[0])\n",
        "            length = len(chars)\n",
        "            char_2d = []\n",
        "            for indexRow in range(max_length):\n",
        "                temp = []\n",
        "                try:\n",
        "                    temp.append(chars[indexRow])\n",
        "                except:\n",
        "                    temp.append(\" \")\n",
        "                char_2d.append(temp)\n",
        "            encoded_row = hot.transform(char_2d).toarray()\n",
        "\n",
        "\n",
        "            encoded_dataset[i] = encoded_row\n",
        "\n",
        "        encoded_datasetLength = len(encoded_dataset[0])\n",
        "        #print(\"encoded_dataset vector lenght\", encoded_datasetLength)\n",
        "        # x = np.array(encoded_dataset)\n",
        "        #print(\"dataset shape\", encoded_dataset.shape)\n",
        "        #print(\"\\n------------------\")\n",
        "        return encoded_dataset, max_length, encoded_datasetLength\n",
        "\n",
        "    def convert_to_int_dataset(self, dataset, dictionary):\n",
        "\n",
        "        row_length = 288\n",
        "        data_length = len(dataset)\n",
        "        int_dataset = np.zeros((data_length, row_length))\n",
        "        padding = len(dictionary)\n",
        "\n",
        "        for index in range(data_length):\n",
        "            text = dataset[index]\n",
        "            row = self.convert_to_int_doc(text, dictionary)\n",
        "            int_dataset[index] = row\n",
        "\n",
        "        return int_dataset\n",
        "\n",
        "    def embedd_doc(self, text, mode):\n",
        "\n",
        "        # mode 0 word embedding , mode 1 one hot , mode 2 integer embedding,3 keras\n",
        "        embedded_vector = []\n",
        "        if mode == 0:\n",
        "            self._number_of_inputs = 140\n",
        "            self._vector_size = 100\n",
        "            embedded_vector, in_vocab, out_vocab = self.embed_doc_word(text)\n",
        "        elif mode == 1:\n",
        "            if self._dictionary is None:\n",
        "                # load dictionary if it's not loaded\n",
        "                try:\n",
        "                    pickle_in = open(\"dict.pickle\", \"rb\")\n",
        "                    self._dictionary = pickle.load(pickle_in)\n",
        "                except:\n",
        "                    #  indicate some error and quit\n",
        "                    return \"no dictionary was found \"\n",
        "            embedded_vector = self.one_hot_encode_doc(self._dictionary,text)\n",
        "        elif mode == 2:\n",
        "            #  integer representation\n",
        "            if self._dictionary is None:\n",
        "                # load dictionary if it's not loaded\n",
        "                try:\n",
        "                    pickle_in = open(\"dict.pickle\", \"rb\")\n",
        "                    self._dictionary = pickle.load(pickle_in)\n",
        "                except:\n",
        "                    #  indicate some error and quit\n",
        "                    return \"no dictionary was found \"\n",
        "            # convert each character to integer number\n",
        "            embedded_vector = self.convert_to_int_doc(text, self._dictionary)\n",
        "        '''elif mode == 3:\n",
        "        #experemental mode , future work\n",
        "            return X, label_binarizer.classes_, one_hot_Y'''\n",
        "        return embedded_vector\n",
        "\n",
        "    def read_dataset(self, mode):\n",
        "        # mode 0 word embedding , mode 1 one hot , mode 2 integer embedding,3 keras\n",
        "\n",
        "        data_df = pd.read_csv(\"/content/GP_ArabicTextEmotionRecognition/GP_PythonFiles/models/Emotional-Tone-Dataset.csv\", encoding=\"windows-1256\")\n",
        "        X = data_df[['tweet']].values\n",
        "        Y = data_df[['label']].values\n",
        "        # use own labels\n",
        "        label_binarizer = LabelBinarizer()\n",
        "        label_binarizer.fit(Y)  # need to be global or remembered to use it later\n",
        "        one_hot_Y = label_binarizer.transform(Y)\n",
        "        if mode == 0:\n",
        "            self._number_of_inputs = 140\n",
        "            self._vector_size = 100\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, one_hot_Y, test_size=self._test_size,\n",
        "                                                                random_state=42)\n",
        "            eX_train, eX_test = self.embed_dataset_word(X_train, X_test)\n",
        "        elif mode == 1:\n",
        "            if self._dictionary is None:\n",
        "                try:\n",
        "                    pickle_in = open(\"dict.pickle\", \"rb\")\n",
        "                    self._dictionary = pickle.load(pickle_in)\n",
        "                except:\n",
        "                    self._dictionary = self.get_dictonary(X)\n",
        "                    pickle_out = open(\"dict.pickle\", \"wb\")\n",
        "                    pickle.dump(self._dictionary, pickle_out)\n",
        "                    pickle_out.close()\n",
        "            X = self.one_hot_encode_dataset(self._dictionary, X)\n",
        "            eX_train, eX_test, y_train, y_test = train_test_split(X, one_hot_Y, test_size=self._test_size,\n",
        "                                                                random_state=42)\n",
        "        elif mode == 2:\n",
        "            if self._dictionary is None:\n",
        "                try:\n",
        "                    pickle_in = open(\"dict.pickle\", \"rb\")\n",
        "                    self._dictionary = pickle.load(pickle_in)\n",
        "                except:\n",
        "                    self._dictionary = self.get_dictonary(X)\n",
        "                    pickle_out = open(\"dict.pickle\", \"wb\")\n",
        "                    pickle.dump(self._dictionary, pickle_out)\n",
        "                    pickle_out.close()\n",
        "\n",
        "            # convert each character to integer number\n",
        "            int_dataset = self.convert_to_int_doc(X, self._dictionary)\n",
        "            eX_train, eX_test, y_train, y_test = train_test_split(int_dataset, one_hot_Y, test_size=self._test_size,\n",
        "                                                                  random_state=42)\n",
        "        elif mode == 3:\n",
        "            # experimental\n",
        "            pass\n",
        "        return eX_train, eX_test, y_train, y_test, label_binarizer.classes_\n",
        "\n",
        "    # add char embedding functions  3 of them  keras one hot integer\n",
        "    # label making function\n",
        "\n",
        "class model:\n",
        "    # import read_data\n",
        "    dir_ = \"/content/GP_ArabicTextEmotionRecognition/GP_PythonFiles/models\"\n",
        "    def __init__(self):\n",
        "        self._dropout_rate = 0.2\n",
        "        self._dropout_rate_softmax = 0.5\n",
        "        self._number_of_inputs = 140  # max number of words /characters per doc(tweet)\n",
        "        self._vector_size = 100  # vector for each word\n",
        "        self._batch_size = 10\n",
        "        self._kernal_size = 3  # An integer or tuple/list of a single integer\n",
        "        self._pool_size = 2\n",
        "        self._epochs = 50\n",
        "        self._test_size = 0.1  # percentage of test from the dataset\n",
        "        self._Learning_rate = 0.0001\n",
        "        # _feature_maps = [100,400,500,600,700,800,900,1000,1100,1200]\n",
        "        self._feature_maps = 100\n",
        "        self._num_conv = 2\n",
        "        self.filename = \"/content/GP_ArabicTextEmotionRecognition/GP_PythonFiles/models/weights.{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}.hdf5\"\n",
        "        self.model = None\n",
        "\n",
        "    def train(self, train_x, train_y, modelname=\"trial\"):\n",
        "        self.model = Sequential()\n",
        "\n",
        "        # input\n",
        "        # self.model.add(keras.layers.Input(shape=(_number_of_inputs,_vector_size)))\n",
        "\n",
        "        # Dropout\n",
        "        self.model.add(keras.layers.Dropout(rate=self._dropout_rate, input_shape=(\n",
        "            self._number_of_inputs, self._vector_size)))  # ,noise_shape,random.randint(0,number_of_inputs)))\n",
        "\n",
        "        # Convolution\n",
        "        self.model.add(\n",
        "            keras.layers.Conv1D(filters=self._vector_size, kernel_size=self._kernal_size, strides=1, activation=\"relu\"))\n",
        "        # self.model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "        for i in range(self._num_conv):\n",
        "            # print(i)\n",
        "            self.model.add(\n",
        "                keras.layers.Conv1D(filters=self._feature_maps, kernel_size=self._kernal_size, strides=1,\n",
        "                                    activation=\"relu\"))\n",
        "            if i % 2 == 0:\n",
        "                # print(\"pool\", i)\n",
        "                #if i % 4 == 0:\n",
        "                    #self.model.add(keras.layers.BatchNormalization())\n",
        "                self.model.add(keras.layers.MaxPooling1D(pool_size=self._pool_size, padding='same'))\n",
        "\n",
        "        # Dropout\n",
        "        self.model.add(\n",
        "            keras.layers.Dropout(self._dropout_rate_softmax))  # ,noise_shape,random.randint(0,number_of_inputs)))\n",
        "\n",
        "        # output\n",
        "        self.model.add(keras.layers.Flatten())\n",
        "        # self.model.add(keras.layers.Dense(500, activation=\"relu\"))\n",
        "        self.model.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
        "\n",
        "        opt = keras.optimizers.Adam(lr=self._Learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                                    amsgrad=False);\n",
        "\n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "        self.model.summary()\n",
        "\n",
        "        tensorboard = TensorBoard(log_dir='./log', histogram_freq=1, write_graph=True, write_grads=True,\n",
        "                                  batch_size=self._batch_size, write_images=True)\n",
        "        Checkpoint = keras.callbacks.ModelCheckpoint(self.filename, monitor='val_acc', verbose=0, save_best_only=True,\n",
        "                                                     save_weights_only=False, mode='auto', period=1)\n",
        "        ReduceLR = keras.callbacks.ReduceLROnPlateau()\n",
        "\n",
        "        # Train the model\n",
        "        self.model.fit(train_x, train_y, batch_size=self._batch_size, epochs=self._epochs, validation_split=0.1,\n",
        "                       shuffle=True, callbacks=[tensorboard, Checkpoint, ReduceLR])\n",
        "        self.model.save(self.dir_ + modelname + \".h5\")\n",
        "\n",
        "    def retrain(self, train_x, train_y, modelname=\"trial\"):\n",
        "        tensorboard = TensorBoard(log_dir='./log', histogram_freq=1, write_graph=True, write_grads=True,\n",
        "                                  batch_size=self._batch_size, write_images=True)\n",
        "        Checkpoint = keras.callbacks.ModelCheckpoint(self.filename, monitor='val_acc', verbose=0, save_best_only=True,\n",
        "                                                     save_weights_only=False, mode='auto', period=1)\n",
        "        ReduceLR = keras.callbacks.ReduceLROnPlateau()\n",
        "\n",
        "        # Train the model\n",
        "        self.model.fit(train_x, train_y, batch_size=self._batch_size, epochs=self._epochs, validation_split=0.1,\n",
        "                       shuffle=True, callbacks=[tensorboard, Checkpoint, ReduceLR])\n",
        "        self.model.save(modelname + \".h5\")\n",
        "\n",
        "    def test(self, test_x, test_y, modelname):\n",
        "        if self.model is None:\n",
        "            try:\n",
        "                self.model = load_model(self.dir_+modelname + \".h5\")\n",
        "            except:\n",
        "                self.model = load_model(self.dir_ + modelname)\n",
        "        self.model.summary()\n",
        "        predicted_y = self.model.predict(test_x)\n",
        "        #print(np.argmax(predicted_y, axis=1).shape)\n",
        "        acc = accuracy_score(np.argmax(test_y, axis=1), np.argmax(predicted_y, axis=1))\n",
        "        report = classification_report(np.argmax(test_y, axis=1), np.argmax(predicted_y, axis=1))\n",
        "        cm = confusion_matrix(np.argmax(test_y, axis=1), np.argmax(predicted_y, axis=1))\n",
        "        return cm, acc, report\n",
        "\n",
        "    def predict(self, sentence, modelname):\n",
        "        if self.model is None:\n",
        "            try:\n",
        "                self.model = load_model(\"/content/GP_ArabicTextEmotionRecognition/GP_PythonFiles/models/c3_100.h5\")\n",
        "            except:\n",
        "                self.model = load_model(\"/content/GP_ArabicTextEmotionRecognition/GP_PythonFiles/models/c3_100\")\n",
        "            predicted_y = self.model.predict(sentence)\n",
        "            # except:\n",
        "            #   print(modelname+\" model not found\")\n",
        "        else:\n",
        "            predicted_y = self.model.predict(sentence)\n",
        "        return predicted_y\n",
        "\n",
        "class system:\n",
        "    dir_ = '/content/GP_ArabicTextEmotionRecognition/GP_PythonFiles/models'\n",
        "\n",
        "    def __init__(self):\n",
        "        self.m = model()\n",
        "        self.data = Data_operations()\n",
        "\n",
        "    def predict_doc(self, text, modelname='3_100', mode=0):  # add default model name\n",
        "        embedded_vector = self.data.embedd_doc(text, mode)\n",
        "        if mode == 0:\n",
        "            arr = np.zeros(shape=(1, embedded_vector.shape[0], embedded_vector.shape[1]))\n",
        "            arr[0] = np.array(embedded_vector)\n",
        "            softmax_prediction = self.m.predict(arr, modelname)\n",
        "        return np.argmax(softmax_prediction)\n",
        "\n",
        "    def add_sample(self, text, lable, filename=\"new_EmotionalTone_dataset.csv\"):  # make sure to add an empty one\n",
        "        row = [text, lable]\n",
        "        with open(filename, 'a') as csvFile:\n",
        "            writer = csv.writer(csvFile)\n",
        "            writer.writerow(row)\n",
        "\n",
        "    def embedd_dataset(self, modelname='trial', mode=0):\n",
        "        X_train, X_test, y_train, y_test, classes_ = self.data.read_dataset(mode)\n",
        "        np.save(self.dir_+'X_train_mode_' + str(mode) + '.npy', X_train)\n",
        "        np.save(self.dir_+'X_test_mode_' + str(mode) + '.npy', X_test)\n",
        "        np.save(self.dir_+'y_train_mode_' + str(mode) + '.npy', y_train)\n",
        "        np.save(self.dir_+'y_test_mode_' + str(mode) + '.npy', y_test)\n",
        "        np.save(self.dir_+\"classes.npy\", classes_)\n",
        "        return X_train, X_test, y_train, y_test, classes_\n",
        "\n",
        "    def train_model(self, modelname='trial', mode=0):\n",
        "        try:\n",
        "            #print(self.dir_+'X_train_mode_' + str(mode) + '.npy')\n",
        "            X_train = np.load(self.dir_+'X_train_mode_' + str(mode) + '.npy')\n",
        "            X_test = np.load(self.dir_+'X_test_mode_' + str(mode) + '.npy')\n",
        "            y_train = np.load(self.dir_+'y_train_mode_' + str(mode) + '.npy')\n",
        "            y_test = np.load(self.dir_+'y_test_mode_' + str(mode) + '.npy')\n",
        "            classes_ = np.load(self.dir_+\"classes.npy\")\n",
        "        except:\n",
        "            X_train, X_test, y_train, y_test, classes_ = self.embedd_dataset(modelname, mode)\n",
        "        if self.m.model is None:\n",
        "            self.m.train(X_train, y_train, modelname)\n",
        "        else:\n",
        "            self.m.retrain(X_train, y_train, modelname)\n",
        "\n",
        "    def test_model(self, modelname='trial', mode=0):\n",
        "        try:\n",
        "            X_test = np.load(self.dir_+'X_test_mode_' + str(mode) + '.npy')\n",
        "            y_test = np.load(self.dir_+'y_test_mode_' + str(mode) + '.npy')\n",
        "            classes_ = np.load(self.dir_+\"classes.npy\")\n",
        "        except:\n",
        "            X_train, X_test, y_train, y_test, classes_ = self.embedd_dataset(modelname, mode)\n",
        "        cm, acc, report = self.m.test(X_test, y_test, modelname)\n",
        "        #print(\"Test Accurcy: \" + str(acc))\n",
        "        #print(report)\n",
        "        plot_confusion_matrix(cm, classes_, True)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRT9AjYRYr8P",
        "cellView": "form"
      },
      "source": [
        "#@title labels\n",
        "def traslate_label(label):\n",
        "  if label == 0:\n",
        "    return  \"غضب\"\n",
        "  if label == 1:\n",
        "    return \"خوف\"\n",
        "  if label == 2:\n",
        "    return \"فرح\"\n",
        "  if label == 3:\n",
        "    return \"حب\"\n",
        "  if label == 4:\n",
        "    return \"طبيعي\"\n",
        "  if label == 5:\n",
        "    return \"حزن\"\n",
        "  if label == 6:\n",
        "    return \"متفاجئ\"\n",
        "  if label == 7:\n",
        "    return \"تعاطف\""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZtIzVNpZXea"
      },
      "source": [
        "# this cell might take longer (first prediction & loading the model)\n",
        "text = \"الان يبدوا انك حقا نسيتني  الم تعد تهتم لامري .. ما كان عليا ان احبك كثيرا \"\n",
        "s = system()\n",
        "lable = s.predict_doc(text)\n",
        "print(traslate_label(lable))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81qc8ZOXYxpo",
        "outputId": "9141265c-26a1-4bf2-d885-bec0750c0438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "text=\"\"\n",
        "while text !=\".\":\n",
        "    text = input()\n",
        "    lable = s.predict_doc(text)\n",
        "    print(traslate_label(lable))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "لالالالا والله ما ينفع اللعب بالاعصاب ده .. ده لو بجد يا جدعان انا يجرالى حاجة من الفرحة\n",
            "حزن\n",
            "بعد فترة انقطاع طويلة رجعنا نرسم لعيونك يا جميل \n",
            "فرح\n",
            "زاي كل المأسي طلعت على مقاييس بالحلاوة و الجمال ده \n",
            "فرح\n",
            "‏من وأنا صغير وحلم حياتي أشتغل شغلانة تضحك الناس .. وترسم الإبتسامة على وشوشهم ، تسعدهم .. بس لما كبرت  .. اكشتفت إن تجارة الحشيش حرام \n",
            "حزن\n",
            "ربنا يهون الكيماوى على مرضى السرطان\n",
            "تعاطف\n",
            "انا مرعوب من امتحان بكرا\n",
            "خوف\n",
            "انا من الناس اللى مش مهتمية انى اتفرج على الاوليمبياد\n",
            "طبيعي\n",
            ".\n",
            "متفاجئ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}