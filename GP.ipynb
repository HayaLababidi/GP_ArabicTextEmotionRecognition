{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GP graphs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayaLababidi/GP_ArabicTextEmotionRecognition/blob/master/GP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c3WpoYvebiLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# get required files from drive & needed installations"
      ]
    },
    {
      "metadata": {
        "id": "IYB_PD0PwzWl",
        "colab_type": "code",
        "outputId": "815dd550-e370-4ecd-be52-1fe93bb9744c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1312
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install sklearn\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install --upgrade google-api-python-client\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.128)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.128 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.128)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.6)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.2)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Requirement already up-to-date: google-api-python-client in /usr/local/lib/python3.6/dist-packages (1.7.8)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.11.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.4)\n",
            "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.5)\n",
            "--2019-04-10 12:15:04--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.203.66.95, 34.226.180.131, 35.172.177.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.203.66.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  85.3MB/s    in 0.2s    \n",
            "\n",
            "2019-04-10 12:15:04 (85.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aIoEjSKXnLgh",
        "colab_type": "code",
        "outputId": "ecfe1735-a5ee-40e9-899c-64fbbd6818d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "#Download all files in gdrive folder\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "#local_download_path = os.path.expanduser('~/data')\n",
        "#try:\n",
        "#  os.makedirs(local_download_path)\n",
        "#except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1j0P7oeHOE8eGhzgsaZFkB6tHTrkr5h95' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = f['title']\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: full_uni_sg_100_twitter.zip, id: 1_yL-V2tzZwRQNjWsippgbaX9632IDP7Q\n",
            "downloading to full_uni_sg_100_twitter.zip\n",
            "title: full_grams_sg_300_twitter.zip, id: 1jJ6-1Y6rFy6cQtZVGa4dJX9tIBY5RNid\n",
            "downloading to full_grams_sg_300_twitter.zip\n",
            "title: full_grams_sg_100_twitter.zip, id: 1gvSKZVeIY3KuHOqiO2-LaFYMWNdPhjDI\n",
            "downloading to full_grams_sg_100_twitter.zip\n",
            "title: full_grams_cbow_300_twitter.zip, id: 1hBdJyZCLpgurOhPYYECWW8SYkfxpykQt\n",
            "downloading to full_grams_cbow_300_twitter.zip\n",
            "title: full_grams_cbow_100_twitter.zip, id: 1W4C3w3dLRvm4t_9uRx8ezgDThTm9jbyO\n",
            "downloading to full_grams_cbow_100_twitter.zip\n",
            "title: full_uni_sg_300_twitter.zip, id: 11Mutc67Re-S-qmpYjx4P9YVUxzxbw35C\n",
            "downloading to full_uni_sg_300_twitter.zip\n",
            "title: full_uni_sg_100_twitter.zip, id: 1zYOhMVGV0X-TTnaRVamrtqiF03foBkWy\n",
            "downloading to full_uni_sg_100_twitter.zip\n",
            "title: full_uni_cbow_100_twitter.zip, id: 1S4v3RMxsqxFXNtsrAHXaL5o3r3hY1lVd\n",
            "downloading to full_uni_cbow_100_twitter.zip\n",
            "title: full_uni_cbow_300_twitter.zip, id: 1H7jzzkHmXHLCO1oDMx4T_yUe5iTRTNl4\n",
            "downloading to full_uni_cbow_300_twitter.zip\n",
            "title: Emotional-Tone-Dataset.csv, id: 1HDgAAPkClv-maZ-xmp_avfewuNeFMg0A\n",
            "downloading to Emotional-Tone-Dataset.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ip7mk9TVrKkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "4e1a5f10-8fc1-4a87-b019-224230991e1d"
      },
      "cell_type": "code",
      "source": [
        "!unzip full_grams_sg_300_twitter.zip\n",
        "!unzip full_grams_sg_100_twitter.zip\n",
        "!unzip full_grams_cbow_300_twitter.zip\n",
        "!unzip full_grams_cbow_100_twitter.zip\n",
        "\n",
        "!unzip full_uni_sg_300_twitter.zip\n",
        "!unzip full_uni_sg_100_twitter.zip\n",
        "!unzip full_uni_cbow_100_twitter.zip\n",
        "!unzip full_uni_cbow_300_twitter.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  full_grams_sg_300_twitter.zip\n",
            "  inflating: full_grams_sg_300_twitter.mdl  \n",
            "  inflating: full_grams_sg_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_sg_300_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_grams_sg_100_twitter.zip\n",
            "  inflating: full_grams_sg_100_twitter.mdl  \n",
            "  inflating: full_grams_sg_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_sg_100_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_grams_cbow_300_twitter.zip\n",
            "  inflating: full_grams_cbow_300_twitter.mdl  \n",
            "  inflating: full_grams_cbow_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_cbow_300_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_grams_cbow_100_twitter.zip\n",
            "  inflating: full_grams_cbow_100_twitter.mdl  \n",
            "  inflating: full_grams_cbow_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_cbow_100_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_uni_sg_300_twitter.zip\n",
            "  inflating: full_uni_sg_300_twitter.mdl  \n",
            "  inflating: full_uni_sg_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_sg_300_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_uni_sg_100_twitter.zip\n",
            "  inflating: full_uni_sg_100_twitter.mdl  \n",
            "  inflating: full_uni_sg_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_sg_100_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_uni_cbow_100_twitter.zip\n",
            "  inflating: full_uni_cbow_100_twitter.mdl  \n",
            "  inflating: full_uni_cbow_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_cbow_100_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_uni_cbow_300_twitter.zip\n",
            "  inflating: full_uni_cbow_300_twitter.mdl  \n",
            "  inflating: full_uni_cbow_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_cbow_300_twitter.mdl.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AmjrnGQGa9mw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***The following code cell was used to download then upload the aravec moels to the drive (Now no longer used)*** \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "i7PqRMkRmhme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://ia802903.us.archive.org/26/items/full_grams_cbow_300_twitter/full_uni_sg_100_twitter.zip'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('full_uni_sg_100_twitter.zip', 'wb').write(r.content)\n",
        "#Download all files in gdrive folder\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "fid = '1j0P7oeHOE8eGhzgsaZFkB6tHTrkr5h95'\n",
        "f = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": fid}]})\n",
        "f.SetContentFile('full_uni_sg_100_twitter.zip')\n",
        "f.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TD5FCNa4vOGz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Read Data , Preprocessing and Embbeding**"
      ]
    },
    {
      "metadata": {
        "id": "G7dmQf8KaIoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer \n",
        "import string\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "\n",
        "_dropout_rate = 0.2\n",
        "_dropout_rate_softmax = 0.5\n",
        "_number_of_inputs = 140 #max number of words /characters per doc(tweet)\n",
        "_vector_size = 300 #vector for each word\n",
        "_batch_size = 100\n",
        "_kernal_size= 5 #An integer or tuple/list of a single integer\n",
        "_pool_size = 3\n",
        "_noise_shape = (_batch_size,1,_number_of_inputs)\n",
        "_epochs = 25\n",
        "_test_size = 0.33 # percentage of test from the dataset\n",
        "_aravec_model_name = \"full_grams_sg_300_twitter\" \n",
        "_Learning_rate = 0.0001\n",
        "\n",
        "my_api_key = \"AIzaSyCUKEOsT6ecC3ods862vgsVOawWyii0NDQ\"\n",
        "my_cse_id = \"007967891901694126580:i3iq-cjlldq\"\n",
        "\n",
        "def google_search(search_term):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "    res = service.cse().list(q=search_term, cx=my_cse_id).execute()\n",
        "    return res['spelling']['correctedQuery']\n",
        "  \n",
        "  \n",
        "#مسح التشكيل و علامات الترقيم و الحروف المتكررة---------\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][،/:\"؟.,'{}~¦+|!”…“–»«•'''\n",
        "english_punctuations = string.punctuation\n",
        "english_numbers = \"0123456789\"\n",
        "punctuations_list = arabic_punctuations + english_punctuations + english_numbers\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                            ـ    | # empty line in between letters (longation) \n",
        "                             ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                         \"\"\", re.VERBOSE)\n",
        "\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"إ\", \"ا\", text)\n",
        "    text = re.sub(\"أ\", \"ا\", text)\n",
        "    text = re.sub(\"آ\", \"ا\", text)\n",
        "    text = re.sub(\"ا\", \"ا\", text)\n",
        "    #text = re.sub(\"ى\", \"ي\", text)\n",
        "    #text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    #text = re.sub(\"ئ\", \"ء\", text)\n",
        "    #text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    text = re.sub(arabic_diacritics, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    for c in punctuations_list:\n",
        "        text = text.replace(c,\" \")\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "#----------------------------------------------------\n",
        "\n",
        "#-------------tokenization and stop word removal\n",
        "def tokens_remove_stopwords(text):\n",
        "\n",
        "    text = text.split()\n",
        "    result = list()\n",
        "    ch = 0\n",
        "\n",
        "    arabic_stop_words = [\"من\", \"فى\", \"الي\", \"علي\", \"عن\", \"حتي\", \"مذ\", \"منذ\", \"و\", \"الا\", \"او\", \"ام\", \"ثم\", \"بل\", \"لكن\",\n",
        "                         \"كل\" , \"متى\" , \"يوم\"]\n",
        "\n",
        "    for word in text:\n",
        "        for stop_word in arabic_stop_words:\n",
        "            if word == stop_word:\n",
        "                ch = 1\n",
        "                break\n",
        "\n",
        "        if ch != 1:\n",
        "            result.append(word)\n",
        "\n",
        "        ch = 0\n",
        "\n",
        "    return result\n",
        "#_______________________________________\n",
        "\n",
        "#Rooting words\n",
        "def rooting(text):\n",
        "    result = list()\n",
        "    for word in text:\n",
        "        stemmer = ISRIStemmer()\n",
        "        result.append(stemmer.stem(word))\n",
        "    return result\n",
        "\n",
        "#remove english and empty strings\n",
        "def remove_english(tokens):\n",
        "    filtered_tokens = list()\n",
        "    for word in tokens:\n",
        "        if (not re.match(r'[a-zA-Z]+', word, re.I)) and word != '':\n",
        "            filtered_tokens.append(word)\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "def preprocess1(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    text = remove_repeating_char(text)\n",
        "    tokens = re.split(\" \", text)\n",
        "    tokens = remove_english(tokens)\n",
        "    return tokens\n",
        "\n",
        "def preprocess2(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    text = remove_repeating_char(text)\n",
        "    text = tokens_remove_stopwords(text)\n",
        "    text = remove_english(text)\n",
        "    text = rooting(text)\n",
        "    return text\n",
        "\n",
        "def embed_doc(text,t_model):\n",
        "    preprocessed_text = preprocess1(text)\n",
        "    #print(preprocessed_text)\n",
        "    \n",
        "    embedded_vectors = np.zeros(shape=(_number_of_inputs,_vector_size))#np array of arrays (array of 100/300 float number per word)\n",
        "    embedded_vectors_index = 0\n",
        "    for i in range(len(preprocessed_text)):\n",
        "        try:\n",
        "            embedded_vectors[embedded_vectors_index] = t_model.wv[preprocessed_text[i]]\n",
        "            embedded_vectors_index = embedded_vectors_index + 1\n",
        "        except:\n",
        "            try:\n",
        "                result = rooting([preprocessed_text[i]])[0]\n",
        "                embedded_vectors[embedded_vectors_index] = t_model.wv[result]\n",
        "                embedded_vectors_index = embedded_vectors_index + 1\n",
        "            except:\n",
        "                try:\n",
        "                    #print(\"in google search \" + preprocessed_text[i])\n",
        "                    search_output = google_search(preprocessed_text[i])\n",
        "                    #print(\"search_output \" + search_output)\n",
        "                    tokens = re.split(\" \", search_output)\n",
        "                    for j in range(len(tokens)):\n",
        "                        try:\n",
        "                            embedded_vectors[embedded_vectors_index] = t_model.wv[tokens[j]]\n",
        "                            embedded_vectors_index = embedded_vectors_index + 1\n",
        "                            print (\"added \" + tokens[j])\n",
        "                        except:\n",
        "                            print(tokens[j] + \" Sub word cant be embedded\")\n",
        "                except:\n",
        "                     print(preprocessed_text[i] + \"word cant be embedded\") #currently emojis can't be embedded and for any extreme case (skip wrongly written words)\n",
        "    return embedded_vectors\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def read_dataset():\n",
        "    t_model = gensim.models.Word2Vec.load(_aravec_model_name +'.mdl')\n",
        "    \n",
        "    data_df = pd.read_csv(\"Emotional-Tone-Dataset.csv\", encoding=\"windows-1256\")\n",
        "    X = data_df[['tweet']].values\n",
        "    Y = data_df[['label']].values\n",
        "    \n",
        "    label_binarizer = LabelBinarizer()\n",
        "    label_binarizer.fit(Y) # need to be global or remembered to use it later\n",
        "    one_hot_Y=label_binarizer.transform(Y)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,one_hot_Y, test_size = _test_size, random_state=42)\n",
        "    \n",
        "    eX_train = np.zeros(shape=(len(X_train),_number_of_inputs,_vector_size))#number of tweets*max number of words per tweet*vector size per word\n",
        "    eX_test = np.zeros(shape=(len(X_test),_number_of_inputs,_vector_size))\n",
        "    \n",
        "    for i in range(len(X_train)):\n",
        "        eX_train[i]= embed_doc(X_train[i],t_model)\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        eX_test[i]= embed_doc(X_test[i],t_model)\n",
        "        \n",
        "    return eX_train, eX_test, y_train, y_test\n",
        "\n",
        "                  \n",
        "X_train, X_test, y_train, y_test = read_dataset()\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('y_test.npy', y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDUR71vLvptn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Model Architecture and Training**"
      ]
    },
    {
      "metadata": {
        "id": "8njH9VNNB1kS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cbefab79-35ba-46f6-c90b-34e607570b3b"
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "# Install\n",
        "! npm install -g localtunnel\n",
        "\n",
        "# Tunnel port 6006 (TensorBoard assumed running)\n",
        "get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')\n",
        "\n",
        "# Get url\n",
        "! cat url.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/client\n",
            "+ localtunnel@1.9.1\n",
            "added 55 packages from 34 contributors in 3.804s\n",
            "your url is: https://good-tiger-83.localtunnel.me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x6I0xxtLw20V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        },
        "outputId": "26ea0f40-09b8-4300-d072-8df38d150872"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "import csv\n",
        "import numpy as np\n",
        "#import read_data\n",
        "_dropout_rate = 0.2\n",
        "_dropout_rate_softmax = 0.5\n",
        "_number_of_inputs = 140 #max number of words /characters per doc(tweet)\n",
        "_vector_size = 300 #vector for each word\n",
        "_batch_size = 10\n",
        "_kernal_size= 5 #An integer or tuple/list of a single integer\n",
        "_pool_size = 3\n",
        "_noise_shape = (_batch_size,1,_number_of_inputs)\n",
        "_epochs = 100\n",
        "_test_size = 0.33 # percentage of test from the dataset\n",
        "_Learning_rate = 0.0001\n",
        "\n",
        "\n",
        "X_train = np.load('X_train.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(\"done input embedding\")\n",
        "model = Sequential()\n",
        "\n",
        "#input\n",
        "#model.add(keras.layers.Input(shape=(_number_of_inputs,_vector_size)))\n",
        "\n",
        "#Dropout\n",
        "#model.add(keras.layers.Dropout(rate=_dropout_rate,input_shape=(_number_of_inputs,_vector_size)))#,noise_shape,random.randint(0,number_of_inputs)))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\",input_shape=(_number_of_inputs,_vector_size)))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "\n",
        "#Dropout\n",
        "model.add(keras.layers.Dropout(_dropout_rate_softmax))#,noise_shape,random.randint(0,number_of_inputs)))\n",
        "\n",
        "#output\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=_Learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False);\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "tensorboard = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=_batch_size,\n",
        "                         write_images=True)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=_batch_size,\n",
        "    epochs=_epochs,\n",
        "    validation_data=(X_test, y_test),\n",
        "    shuffle=True,\n",
        "    callbacks=[tensorboard]\n",
        ")\n",
        "model.save(\"trail_9.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6743, 140, 300)\n",
            "(6743, 8)\n",
            "(3322, 140, 300)\n",
            "(3322, 8)\n",
            "done input embedding\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_21 (Conv1D)           (None, 140, 300)          450300    \n",
            "_________________________________________________________________\n",
            "conv1d_22 (Conv1D)           (None, 140, 300)          450300    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling (None, 47, 300)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_23 (Conv1D)           (None, 47, 300)           450300    \n",
            "_________________________________________________________________\n",
            "conv1d_24 (Conv1D)           (None, 47, 300)           450300    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling (None, 16, 300)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_25 (Conv1D)           (None, 16, 300)           450300    \n",
            "_________________________________________________________________\n",
            "conv1d_26 (Conv1D)           (None, 16, 300)           450300    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, 6, 300)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_27 (Conv1D)           (None, 6, 300)            450300    \n",
            "_________________________________________________________________\n",
            "conv1d_28 (Conv1D)           (None, 6, 300)            450300    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling (None, 2, 300)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_29 (Conv1D)           (None, 2, 300)            450300    \n",
            "_________________________________________________________________\n",
            "conv1d_30 (Conv1D)           (None, 2, 300)            450300    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_15 (MaxPooling (None, 1, 300)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1, 300)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 2408      \n",
            "=================================================================\n",
            "Total params: 4,505,408\n",
            "Trainable params: 4,505,408\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 6743 samples, validate on 3322 samples\n",
            "Epoch 1/100\n",
            "6743/6743 [==============================] - 46s 7ms/step - loss: 1.8315 - acc: 0.2834 - val_loss: 1.5941 - val_acc: 0.3820\n",
            "Epoch 2/100\n",
            "6743/6743 [==============================] - 39s 6ms/step - loss: 1.4566 - acc: 0.4919 - val_loss: 1.2328 - val_acc: 0.5810\n",
            "Epoch 3/100\n",
            "6743/6743 [==============================] - 37s 5ms/step - loss: 1.0730 - acc: 0.6367 - val_loss: 1.0570 - val_acc: 0.6391\n",
            "Epoch 4/100\n",
            "6743/6743 [==============================] - 37s 5ms/step - loss: 0.8280 - acc: 0.7222 - val_loss: 1.2190 - val_acc: 0.6186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k3Mlv4kbI8ml",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Upload the Saved the model (architecture + weights + optimizer state)**"
      ]
    },
    {
      "metadata": {
        "id": "9EN7oYecEc3G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Download all files in gdrive folder\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "model.save(\"trail_6.h5\")\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "fid = '1abRQMGnEtANpAop4JHsvL2PjF6XNoaPU'\n",
        "f = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": fid}]})\n",
        "f.SetContentFile('trail_9.h5')\n",
        "f.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}