{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayaLababidi/GP_ArabicTextEmotionRecognition/blob/master/GP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "IYB_PD0PwzWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install sklearn\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install --upgrade google-api-python-client"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3WpoYvebiLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# get required files from drive"
      ]
    },
    {
      "metadata": {
        "id": "aIoEjSKXnLgh",
        "colab_type": "code",
        "outputId": "9939e4f4-4984-4d89-9fee-0c785f9e96c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "#Download all files in gdrive folder\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "#local_download_path = os.path.expanduser('~/data')\n",
        "#try:\n",
        "#  os.makedirs(local_download_path)\n",
        "#except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1j0P7oeHOE8eGhzgsaZFkB6tHTrkr5h95' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = f['title']\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: full_grams_sg_300_twitter.zip, id: 1jJ6-1Y6rFy6cQtZVGa4dJX9tIBY5RNid\n",
            "downloading to full_grams_sg_300_twitter.zip\n",
            "title: full_grams_sg_100_twitter.zip, id: 1gvSKZVeIY3KuHOqiO2-LaFYMWNdPhjDI\n",
            "downloading to full_grams_sg_100_twitter.zip\n",
            "title: full_grams_cbow_300_twitter.zip, id: 1hBdJyZCLpgurOhPYYECWW8SYkfxpykQt\n",
            "downloading to full_grams_cbow_300_twitter.zip\n",
            "title: full_grams_cbow_100_twitter.zip, id: 1W4C3w3dLRvm4t_9uRx8ezgDThTm9jbyO\n",
            "downloading to full_grams_cbow_100_twitter.zip\n",
            "title: full_uni_sg_300_twitter.zip, id: 11Mutc67Re-S-qmpYjx4P9YVUxzxbw35C\n",
            "downloading to full_uni_sg_300_twitter.zip\n",
            "title: full_uni_sg_100_twitter.zip, id: 1zYOhMVGV0X-TTnaRVamrtqiF03foBkWy\n",
            "downloading to full_uni_sg_100_twitter.zip\n",
            "title: full_uni_cbow_100_twitter.zip, id: 1S4v3RMxsqxFXNtsrAHXaL5o3r3hY1lVd\n",
            "downloading to full_uni_cbow_100_twitter.zip\n",
            "title: full_uni_cbow_300_twitter.zip, id: 1H7jzzkHmXHLCO1oDMx4T_yUe5iTRTNl4\n",
            "downloading to full_uni_cbow_300_twitter.zip\n",
            "title: Emotional-Tone-Dataset.csv, id: 1HDgAAPkClv-maZ-xmp_avfewuNeFMg0A\n",
            "downloading to Emotional-Tone-Dataset.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ip7mk9TVrKkh",
        "colab_type": "code",
        "outputId": "ffaf1678-aad5-4419-db4e-473d9bbda93c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip full_grams_sg_300_twitter.zip\n",
        "!unzip full_grams_sg_100_twitter.zip\n",
        "!unzip full_grams_cbow_300_twitter.zip\n",
        "!unzip full_grams_cbow_100_twitter.zip\n",
        "\n",
        "!unzip full_uni_sg_300_twitter.zip\n",
        "!unzip full_uni_sg_100_twitter.zip\n",
        "!unzip full_uni_cbow_100_twitter.zip\n",
        "!unzip full_uni_cbow_300_twitter.zip"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  full_grams_sg_300_twitter.zip\n",
            "  inflating: full_grams_sg_300_twitter.mdl  \n",
            "  inflating: full_grams_sg_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_sg_300_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_grams_sg_100_twitter.zip\n",
            "  inflating: full_grams_sg_100_twitter.mdl  \n",
            "  inflating: full_grams_sg_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_sg_100_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_grams_cbow_300_twitter.zip\n",
            "  inflating: full_grams_cbow_300_twitter.mdl  \n",
            "  inflating: full_grams_cbow_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_cbow_300_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_grams_cbow_100_twitter.zip\n",
            "  inflating: full_grams_cbow_100_twitter.mdl  \n",
            "  inflating: full_grams_cbow_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_cbow_100_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_uni_sg_300_twitter.zip\n",
            "  inflating: full_uni_sg_300_twitter.mdl  \n",
            "  inflating: full_uni_sg_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_sg_300_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_uni_sg_100_twitter.zip\n",
            "  inflating: full_uni_sg_100_twitter.mdl  \n",
            "  inflating: full_uni_sg_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_sg_100_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_uni_cbow_100_twitter.zip\n",
            "  inflating: full_uni_cbow_100_twitter.mdl  \n",
            "  inflating: full_uni_cbow_100_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_cbow_100_twitter.mdl.wv.vectors.npy  \n",
            "Archive:  full_uni_cbow_300_twitter.zip\n",
            "  inflating: full_uni_cbow_300_twitter.mdl  \n",
            "  inflating: full_uni_cbow_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_cbow_300_twitter.mdl.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AmjrnGQGa9mw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***The following code cell was used to upload the aravec moels to the drive (Now no longer used)*** \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "i7PqRMkRmhme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://ia802903.us.archive.org/26/items/full_grams_cbow_300_twitter/full_uni_sg_100_twitter.zip'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('full_uni_sg_100_twitter.zip', 'wb').write(r.content)\n",
        "#Download all files in gdrive folder\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "fid = '1j0P7oeHOE8eGhzgsaZFkB6tHTrkr5h95'\n",
        "f = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": fid}]})\n",
        "f.SetContentFile('full_uni_sg_100_twitter.zip')\n",
        "f.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TD5FCNa4vOGz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Read Data , Preprocessing and Embbeding**"
      ]
    },
    {
      "metadata": {
        "id": "G7dmQf8KaIoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "\n",
        "_dropout_rate = 0.2\n",
        "_dropout_rate_softmax = 0.5\n",
        "_number_of_inputs = 140 #max number of words /characters per doc(tweet)\n",
        "_vector_size = 300 #vector for each word\n",
        "_batch_size = 100\n",
        "_kernal_size= 5 #An integer or tuple/list of a single integer\n",
        "_pool_size = 3\n",
        "_noise_shape = (_batch_size,1,_number_of_inputs)\n",
        "_epochs = 25\n",
        "_test_size = 0.33 # percentage of test from the dataset\n",
        "_aravec_model_name = \"full_grams_sg_300_twitter\" \n",
        "_Learning_rate = 0.0001\n",
        "\n",
        "my_api_key = \"AIzaSyCUKEOsT6ecC3ods862vgsVOawWyii0NDQ\"\n",
        "my_cse_id = \"007967891901694126580:i3iq-cjlldq\"\n",
        "\n",
        "def google_search(search_term):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "    res = service.cse().list(q=search_term, cx=my_cse_id).execute()\n",
        "    return res['spelling']['correctedQuery']\n",
        "\n",
        "\n",
        "\n",
        "def embed_label(label):\n",
        "  if label == \"anger\":\n",
        "    return 0\n",
        "  if label == \"joy\":\n",
        "    return 1\n",
        "  if label == \"none\":\n",
        "    return 2\n",
        "  if label == \"surprise\":\n",
        "    return 3\n",
        "  if label == \"sadness\":\n",
        "    return 4\n",
        "  if label == \"fear\":\n",
        "    return 5\n",
        "  if label == \"sympathy\":\n",
        "    return 6\n",
        "  if label == \"love\":\n",
        "    return 7\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "#text = input(\"enter arabic text\")\n",
        "\n",
        "#مسح التشكيل و علامات الترقيم و الحروف المتكررة---------\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "english_numbers = \"0123456789\"\n",
        "punctuations_list = arabic_punctuations + english_punctuations + english_numbers\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                         \"\"\", re.VERBOSE)\n",
        "\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"إ\", \"ا\", text)\n",
        "    text = re.sub(\"أ\", \"ا\", text)\n",
        "    text = re.sub(\"آ\", \"ا\", text)\n",
        "    text = re.sub(\"ا\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    text = re.sub(arabic_diacritics, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "#----------------------------------------------------\n",
        "\n",
        "#-------------tokenization and stop word removal\n",
        "def tokens_remove_stopwords(text):\n",
        "\n",
        "    text = text.split()\n",
        "    result = list()\n",
        "    ch = 0\n",
        "\n",
        "    arabic_stop_words = [\"من\", \"فى\", \"الي\", \"علي\", \"عن\", \"حتي\", \"مذ\", \"منذ\", \"و\", \"الا\", \"او\", \"ام\", \"ثم\", \"بل\", \"لكن\",\n",
        "                         \"كل\" , \"متى\" , \"يوم\"]\n",
        "\n",
        "    for word in text:\n",
        "        for stop_word in arabic_stop_words:\n",
        "            if word == stop_word:\n",
        "                ch = 1\n",
        "                break\n",
        "\n",
        "        if ch != 1:\n",
        "            result.append(word)\n",
        "\n",
        "        ch = 0\n",
        "\n",
        "    return result\n",
        "#_______________________________________\n",
        "\n",
        "#Rooting words\n",
        "def rooting(text):\n",
        "    result = list()\n",
        "    for word in text:\n",
        "        stemmer = ISRIStemmer()\n",
        "        result.append(stemmer.stem(word))\n",
        "    return result\n",
        "\n",
        "#remove english and empty strings\n",
        "def remove_english(tokens):\n",
        "    filtered_tokens = list()\n",
        "    for word in tokens:\n",
        "        if (not re.match(r'[a-zA-Z]+', word, re.I)) and word != '':\n",
        "            filtered_tokens.append(word)\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "def preprocess1(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    text = remove_repeating_char(text)\n",
        "    tokens = re.split(\" \", text)\n",
        "    tokens = remove_english(tokens)\n",
        "    return tokens\n",
        "\n",
        "def preprocess2(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    text = remove_repeating_char(text)\n",
        "    text = tokens_remove_stopwords(text)\n",
        "    text = remove_english(text)\n",
        "    text = rooting(text)\n",
        "    return text\n",
        "\n",
        "'''\n",
        "text = normalize_arabic(text)\n",
        "text = remove_diacritics(text)\n",
        "text = remove_punctuations(text)\n",
        "text = remove_repeating_char(text)\n",
        "\n",
        "\n",
        "print(text)\n",
        "'''\n",
        "def embed_doc(text,t_model):\n",
        "    preprocessed_text = preprocess1(text)\n",
        "    #print(preprocessed_text)\n",
        "    \n",
        "    embedded_vectors = np.zeros(shape=(_number_of_inputs,_vector_size))#np array of arrays (array of 100/300 float number per word)\n",
        "    embedded_vectors_index = 0\n",
        "    for i in range(len(preprocessed_text)):\n",
        "        try:\n",
        "            embedded_vectors[embedded_vectors_index] = t_model.wv[preprocessed_text[i]]\n",
        "            embedded_vectors_index = embedded_vectors_index + 1\n",
        "        except:\n",
        "            try:\n",
        "                result = rooting([preprocessed_text[i]])[0]\n",
        "                embedded_vectors[embedded_vectors_index] = t_model.wv[result]\n",
        "                embedded_vectors_index = embedded_vectors_index + 1\n",
        "            except:\n",
        "                try:\n",
        "                    search_output = google_search(preprocessed_text[i])\n",
        "                    tokens = re.split(\" \", search_output)\n",
        "                    for j in range(len(tokens)):\n",
        "                        try:\n",
        "                            embedded_vectors[embedded_vectors_index] = t_model.wv[tokens[j]]\n",
        "                            embedded_vectors_index = embedded_vectors_index + 1\n",
        "                        except:\n",
        "                            print(tokens[j] + \" Sub word cant be embedded\")\n",
        "                except:\n",
        "                     print(preprocessed_text[i] + \"word cant be embedded\") #currently emojis can't be embedded and for any extreme case (skip wrongly written words)\n",
        "    return embedded_vectors\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def read_dataset():\n",
        "    t_model = gensim.models.Word2Vec.load(_aravec_model_name +'.mdl')\n",
        "    \n",
        "    data_df = pd.read_csv(\"Emotional-Tone-Dataset.csv\", encoding=\"windows-1256\")\n",
        "    X = data_df[['tweet']].values\n",
        "    Y = data_df[['label']].values\n",
        "    \n",
        "    eY = np.zeros(shape=(len(Y)))\n",
        "    for i in range(len(Y)):\n",
        "        eY[i]= embed_label(Y[i])\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,eY, test_size = _test_size, random_state=42)\n",
        "    \n",
        "    eX_train = np.zeros(shape=(len(X_train),_number_of_inputs,_vector_size))#number of tweets*max number of words per tweet*vector size per word\n",
        "    eX_test = np.zeros(shape=(len(X_test),_number_of_inputs,_vector_size))\n",
        "    \n",
        "    for i in range(len(X_train)):\n",
        "        eX_train[i]= embed_doc(X_train[i],t_model)\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        eX_test[i]= embed_doc(X_test[i],t_model)\n",
        "        \n",
        "    return eX_train, eX_test, y_train, y_test\n",
        "\n",
        "                  \n",
        "X_train, X_test, y_train, y_test = read_dataset()\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('y_test.npy', y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDUR71vLvptn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Model Architecture and Training**"
      ]
    },
    {
      "metadata": {
        "id": "x6I0xxtLw20V",
        "colab_type": "code",
        "outputId": "4d3bd1ae-da77-40be-c70c-1babca7c9e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4321
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "import csv\n",
        "import numpy as np\n",
        "#import read_data\n",
        "_dropout_rate = 0.2\n",
        "_dropout_rate_softmax = 0.5\n",
        "_number_of_inputs = 140 #max number of words /characters per doc(tweet)\n",
        "_vector_size = 300 #vector for each word\n",
        "_batch_size = 10\n",
        "_kernal_size= 5 #An integer or tuple/list of a single integer\n",
        "_pool_size = 3\n",
        "_noise_shape = (_batch_size,1,_number_of_inputs)\n",
        "_epochs = 100\n",
        "_test_size = 0.33 # percentage of test from the dataset\n",
        "_Learning_rate = 0.0001\n",
        "\n",
        "\n",
        "X_train = np.load('X_train.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(\"done input embedding\")\n",
        "model = Sequential()\n",
        "\n",
        "#input\n",
        "#model.add(keras.layers.Input(shape=(_number_of_inputs,_vector_size)))\n",
        "\n",
        "#Dropout\n",
        "model.add(keras.layers.Dropout(rate=_dropout_rate,input_shape=(_number_of_inputs,_vector_size)))#,noise_shape,random.randint(0,number_of_inputs)))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "#Convolution\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "\n",
        "#Dropout\n",
        "model.add(keras.layers.Dropout(_dropout_rate_softmax))#,noise_shape,random.randint(0,number_of_inputs)))\n",
        "\n",
        "#output\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=_Learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False);\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=_batch_size,\n",
        "    epochs=_epochs,\n",
        "    validation_data=(X_test, y_test),\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6743, 140, 100)\n",
            "(6743,)\n",
            "(3322, 140, 100)\n",
            "(3322,)\n",
            "done input embedding\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_5 (Dropout)          (None, 140, 100)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_21 (Conv1D)           (None, 140, 100)          50100     \n",
            "_________________________________________________________________\n",
            "conv1d_22 (Conv1D)           (None, 140, 100)          50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling (None, 47, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_23 (Conv1D)           (None, 47, 100)           50100     \n",
            "_________________________________________________________________\n",
            "conv1d_24 (Conv1D)           (None, 47, 100)           50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling (None, 16, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_25 (Conv1D)           (None, 16, 100)           50100     \n",
            "_________________________________________________________________\n",
            "conv1d_26 (Conv1D)           (None, 16, 100)           50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, 6, 100)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_27 (Conv1D)           (None, 6, 100)            50100     \n",
            "_________________________________________________________________\n",
            "conv1d_28 (Conv1D)           (None, 6, 100)            50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling (None, 2, 100)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_29 (Conv1D)           (None, 2, 100)            50100     \n",
            "_________________________________________________________________\n",
            "conv1d_30 (Conv1D)           (None, 2, 100)            50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_15 (MaxPooling (None, 1, 100)            0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1, 100)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 808       \n",
            "=================================================================\n",
            "Total params: 501,808\n",
            "Trainable params: 501,808\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6743 samples, validate on 3322 samples\n",
            "Epoch 1/100\n",
            "6743/6743 [==============================] - 12s 2ms/step - loss: 1.9481 - acc: 0.2171 - val_loss: 1.7478 - val_acc: 0.3227\n",
            "Epoch 2/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.6735 - acc: 0.3940 - val_loss: 1.4648 - val_acc: 0.4771\n",
            "Epoch 3/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.4637 - acc: 0.4892 - val_loss: 1.3331 - val_acc: 0.5226\n",
            "Epoch 4/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.3786 - acc: 0.5281 - val_loss: 1.3182 - val_acc: 0.5449\n",
            "Epoch 5/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.3228 - acc: 0.5541 - val_loss: 1.2767 - val_acc: 0.5542\n",
            "Epoch 6/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.2389 - acc: 0.5810 - val_loss: 1.2847 - val_acc: 0.5653\n",
            "Epoch 7/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.1980 - acc: 0.5959 - val_loss: 1.1820 - val_acc: 0.5933\n",
            "Epoch 8/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.1414 - acc: 0.6152 - val_loss: 1.0883 - val_acc: 0.6237\n",
            "Epoch 9/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.0846 - acc: 0.6364 - val_loss: 1.0855 - val_acc: 0.6264\n",
            "Epoch 10/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.0602 - acc: 0.6453 - val_loss: 1.0853 - val_acc: 0.6243\n",
            "Epoch 11/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 1.0095 - acc: 0.6638 - val_loss: 1.0503 - val_acc: 0.6358\n",
            "Epoch 12/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.9772 - acc: 0.6723 - val_loss: 1.0687 - val_acc: 0.6346\n",
            "Epoch 13/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.9420 - acc: 0.6874 - val_loss: 1.0675 - val_acc: 0.6352\n",
            "Epoch 14/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.8854 - acc: 0.7111 - val_loss: 1.0489 - val_acc: 0.6520\n",
            "Epoch 15/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.8763 - acc: 0.7053 - val_loss: 1.0386 - val_acc: 0.6496\n",
            "Epoch 16/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.8180 - acc: 0.7298 - val_loss: 1.0700 - val_acc: 0.6445\n",
            "Epoch 17/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.7974 - acc: 0.7342 - val_loss: 1.0806 - val_acc: 0.6364\n",
            "Epoch 18/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.7653 - acc: 0.7489 - val_loss: 1.1123 - val_acc: 0.6460\n",
            "Epoch 19/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.7390 - acc: 0.7557 - val_loss: 1.0934 - val_acc: 0.6397\n",
            "Epoch 20/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.7017 - acc: 0.7684 - val_loss: 1.1064 - val_acc: 0.6388\n",
            "Epoch 21/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.6856 - acc: 0.7743 - val_loss: 1.1312 - val_acc: 0.6391\n",
            "Epoch 22/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.6643 - acc: 0.7830 - val_loss: 1.1875 - val_acc: 0.6364\n",
            "Epoch 23/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.6391 - acc: 0.7939 - val_loss: 1.1911 - val_acc: 0.6189\n",
            "Epoch 24/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.6350 - acc: 0.7955 - val_loss: 1.1839 - val_acc: 0.6352\n",
            "Epoch 25/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.5980 - acc: 0.7989 - val_loss: 1.1778 - val_acc: 0.6511\n",
            "Epoch 26/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.5792 - acc: 0.8087 - val_loss: 1.2593 - val_acc: 0.6141\n",
            "Epoch 27/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.5523 - acc: 0.8200 - val_loss: 1.1959 - val_acc: 0.6460\n",
            "Epoch 28/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.5494 - acc: 0.8220 - val_loss: 1.2326 - val_acc: 0.6231\n",
            "Epoch 29/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.5218 - acc: 0.8241 - val_loss: 1.2438 - val_acc: 0.6312\n",
            "Epoch 30/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.5184 - acc: 0.8287 - val_loss: 1.2757 - val_acc: 0.6243\n",
            "Epoch 31/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.4919 - acc: 0.8376 - val_loss: 1.2482 - val_acc: 0.6400\n",
            "Epoch 32/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.4718 - acc: 0.8419 - val_loss: 1.2884 - val_acc: 0.6532\n",
            "Epoch 33/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.4674 - acc: 0.8502 - val_loss: 1.3245 - val_acc: 0.6258\n",
            "Epoch 34/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.4557 - acc: 0.8523 - val_loss: 1.4676 - val_acc: 0.6045\n",
            "Epoch 35/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.4174 - acc: 0.8597 - val_loss: 1.3342 - val_acc: 0.6364\n",
            "Epoch 36/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.4172 - acc: 0.8633 - val_loss: 1.4348 - val_acc: 0.6355\n",
            "Epoch 37/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.4007 - acc: 0.8676 - val_loss: 1.4208 - val_acc: 0.6385\n",
            "Epoch 38/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3847 - acc: 0.8714 - val_loss: 1.3885 - val_acc: 0.6168\n",
            "Epoch 39/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3823 - acc: 0.8753 - val_loss: 1.4416 - val_acc: 0.6276\n",
            "Epoch 40/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3614 - acc: 0.8790 - val_loss: 1.5769 - val_acc: 0.6147\n",
            "Epoch 41/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3741 - acc: 0.8768 - val_loss: 1.5544 - val_acc: 0.6087\n",
            "Epoch 42/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3544 - acc: 0.8827 - val_loss: 1.3346 - val_acc: 0.6349\n",
            "Epoch 43/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3349 - acc: 0.8886 - val_loss: 1.5087 - val_acc: 0.6463\n",
            "Epoch 44/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3469 - acc: 0.8858 - val_loss: 1.4394 - val_acc: 0.6385\n",
            "Epoch 45/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3163 - acc: 0.8944 - val_loss: 1.4561 - val_acc: 0.6340\n",
            "Epoch 46/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3173 - acc: 0.8972 - val_loss: 1.4686 - val_acc: 0.6273\n",
            "Epoch 47/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3192 - acc: 0.8935 - val_loss: 1.5555 - val_acc: 0.6147\n",
            "Epoch 48/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.3063 - acc: 0.8995 - val_loss: 1.5652 - val_acc: 0.6042\n",
            "Epoch 49/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2944 - acc: 0.9005 - val_loss: 1.6665 - val_acc: 0.6159\n",
            "Epoch 50/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2788 - acc: 0.9045 - val_loss: 1.6435 - val_acc: 0.6204\n",
            "Epoch 51/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2608 - acc: 0.9132 - val_loss: 1.7128 - val_acc: 0.6219\n",
            "Epoch 52/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2534 - acc: 0.9118 - val_loss: 1.5865 - val_acc: 0.6267\n",
            "Epoch 53/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2562 - acc: 0.9122 - val_loss: 1.5939 - val_acc: 0.6373\n",
            "Epoch 54/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2531 - acc: 0.9170 - val_loss: 1.6364 - val_acc: 0.6288\n",
            "Epoch 55/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2379 - acc: 0.9233 - val_loss: 1.7870 - val_acc: 0.6168\n",
            "Epoch 56/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2452 - acc: 0.9204 - val_loss: 1.5756 - val_acc: 0.6415\n",
            "Epoch 57/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2262 - acc: 0.9275 - val_loss: 1.6618 - val_acc: 0.6436\n",
            "Epoch 58/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2213 - acc: 0.9293 - val_loss: 1.6975 - val_acc: 0.6225\n",
            "Epoch 59/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2162 - acc: 0.9287 - val_loss: 1.8101 - val_acc: 0.6276\n",
            "Epoch 60/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.2305 - acc: 0.9258 - val_loss: 1.6246 - val_acc: 0.6364\n",
            "Epoch 61/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1978 - acc: 0.9367 - val_loss: 1.7937 - val_acc: 0.6174\n",
            "Epoch 62/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1938 - acc: 0.9346 - val_loss: 1.9042 - val_acc: 0.6207\n",
            "Epoch 63/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1959 - acc: 0.9377 - val_loss: 1.7014 - val_acc: 0.6460\n",
            "Epoch 64/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1935 - acc: 0.9368 - val_loss: 1.8411 - val_acc: 0.6141\n",
            "Epoch 65/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1785 - acc: 0.9453 - val_loss: 1.9020 - val_acc: 0.5960\n",
            "Epoch 66/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1719 - acc: 0.9465 - val_loss: 2.0626 - val_acc: 0.6261\n",
            "Epoch 67/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1682 - acc: 0.9478 - val_loss: 1.8249 - val_acc: 0.6249\n",
            "Epoch 68/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1617 - acc: 0.9475 - val_loss: 1.7410 - val_acc: 0.6442\n",
            "Epoch 69/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1652 - acc: 0.9503 - val_loss: 1.8418 - val_acc: 0.6364\n",
            "Epoch 70/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1675 - acc: 0.9468 - val_loss: 1.8254 - val_acc: 0.6382\n",
            "Epoch 71/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1614 - acc: 0.9479 - val_loss: 1.7985 - val_acc: 0.6334\n",
            "Epoch 72/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1529 - acc: 0.9531 - val_loss: 1.7505 - val_acc: 0.6228\n",
            "Epoch 73/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1420 - acc: 0.9548 - val_loss: 1.8479 - val_acc: 0.6382\n",
            "Epoch 74/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1362 - acc: 0.9545 - val_loss: 1.9492 - val_acc: 0.6309\n",
            "Epoch 75/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1526 - acc: 0.9546 - val_loss: 1.9120 - val_acc: 0.6219\n",
            "Epoch 76/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1391 - acc: 0.9536 - val_loss: 2.0361 - val_acc: 0.6373\n",
            "Epoch 77/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1447 - acc: 0.9543 - val_loss: 1.9013 - val_acc: 0.6240\n",
            "Epoch 78/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1333 - acc: 0.9563 - val_loss: 1.9388 - val_acc: 0.6400\n",
            "Epoch 79/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1311 - acc: 0.9579 - val_loss: 1.9508 - val_acc: 0.6114\n",
            "Epoch 80/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1304 - acc: 0.9582 - val_loss: 2.0983 - val_acc: 0.6334\n",
            "Epoch 81/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1220 - acc: 0.9614 - val_loss: 2.0965 - val_acc: 0.6198\n",
            "Epoch 82/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1303 - acc: 0.9603 - val_loss: 1.8257 - val_acc: 0.6364\n",
            "Epoch 83/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1197 - acc: 0.9629 - val_loss: 2.1798 - val_acc: 0.6222\n",
            "Epoch 84/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1366 - acc: 0.9588 - val_loss: 2.2601 - val_acc: 0.6294\n",
            "Epoch 85/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1138 - acc: 0.9654 - val_loss: 2.2400 - val_acc: 0.6002\n",
            "Epoch 86/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1212 - acc: 0.9640 - val_loss: 2.0053 - val_acc: 0.6279\n",
            "Epoch 87/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1132 - acc: 0.9647 - val_loss: 2.0403 - val_acc: 0.6207\n",
            "Epoch 88/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1070 - acc: 0.9662 - val_loss: 1.8655 - val_acc: 0.6267\n",
            "Epoch 89/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1166 - acc: 0.9620 - val_loss: 2.1340 - val_acc: 0.6207\n",
            "Epoch 90/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1100 - acc: 0.9674 - val_loss: 2.0032 - val_acc: 0.6234\n",
            "Epoch 91/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1091 - acc: 0.9659 - val_loss: 1.9088 - val_acc: 0.6318\n",
            "Epoch 92/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.0917 - acc: 0.9700 - val_loss: 2.2638 - val_acc: 0.6120\n",
            "Epoch 93/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1070 - acc: 0.9653 - val_loss: 1.9801 - val_acc: 0.6379\n",
            "Epoch 94/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.1080 - acc: 0.9665 - val_loss: 2.0692 - val_acc: 0.6174\n",
            "Epoch 95/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.0902 - acc: 0.9696 - val_loss: 2.2549 - val_acc: 0.6252\n",
            "Epoch 96/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.0939 - acc: 0.9693 - val_loss: 2.1822 - val_acc: 0.6120\n",
            "Epoch 97/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.0963 - acc: 0.9671 - val_loss: 2.1778 - val_acc: 0.6258\n",
            "Epoch 98/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.0953 - acc: 0.9686 - val_loss: 2.1248 - val_acc: 0.6406\n",
            "Epoch 99/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.0912 - acc: 0.9714 - val_loss: 1.9992 - val_acc: 0.6270\n",
            "Epoch 100/100\n",
            "6743/6743 [==============================] - 11s 2ms/step - loss: 0.0953 - acc: 0.9693 - val_loss: 2.2944 - val_acc: 0.6096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb2b15765c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}