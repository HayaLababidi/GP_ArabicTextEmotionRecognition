{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GP char embedding (usingembeddinglayer) .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayaLababidi/GP_ArabicTextEmotionRecognition/blob/master/GP_char_embedding_(usingembeddinglayer)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzgTzx-0Lru4",
        "colab_type": "text"
      },
      "source": [
        "# Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYB_PD0PwzWl",
        "colab_type": "code",
        "outputId": "d3037159-ad64-4454-ed8f-cc4f5beb0b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        }
      },
      "source": [
        "!pip install gensim\n",
        "!pip install sklearn\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install --upgrade google-api-python-client"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.147)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.147 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.147)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.147->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.147->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.20.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.16.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.16.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.7)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already up-to-date: google-api-python-client in /usr/local/lib/python3.6/dist-packages (1.7.8)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.3)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.11.3)\n",
            "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3WpoYvebiLH",
        "colab_type": "text"
      },
      "source": [
        "# get required files from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esv7HYa2cUqD",
        "colab_type": "code",
        "outputId": "169e657a-3bbc-41a9-9841-68d0f83824f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Download all files in gdrive folder\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer \n",
        "import string\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import keras\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Conv1D ,MaxPooling1D , Embedding\n",
        "from keras.engine.input_layer import Input\n",
        "\n",
        "import csv\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIoEjSKXnLgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#charecter embedding files\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1rdLWzamDZgoZJ7NkPY74zHqa5xTX-5LR' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = f['title']\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  print( f['id'])\n",
        "  f_.GetContentFile(fname)\n",
        "\n",
        "f_ = drive.CreateFile({'id': \"1HDgAAPkClv-maZ-xmp_avfewuNeFMg0A\"})\n",
        "fname = \"Emotional-Tone-Dataset.csv\"\n",
        "f_.GetContentFile(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfdOlDM4zGG",
        "colab_type": "text"
      },
      "source": [
        "# Read DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-e4dJJs4wt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_dataset():\n",
        "    data_df = pd.read_csv(\"Emotional-Tone-Dataset.csv\", encoding=\"windows-1256\")\n",
        "    X = data_df[['tweet']].values\n",
        "    Y = data_df[['label']].values\n",
        "    \n",
        "    label_binarizer = LabelBinarizer()\n",
        "    label_binarizer.fit(Y) # need to be global or remembered to use it later\n",
        "    one_hot_Y=label_binarizer.transform(Y)\n",
        "    \n",
        "    return X, one_hot_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-jc3SJ95a8x",
        "colab_type": "text"
      },
      "source": [
        "# convert each character to integer number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6idLXZPwWo4H",
        "colab_type": "code",
        "outputId": "2705bdc2-7a27-4c8d-d0cf-6120597103b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def get_dictonary(dataset):\n",
        "    uniques = ''\n",
        "    row = ''\n",
        "    for text in dataset:\n",
        "        #row = ''\n",
        "        try:\n",
        "           row =row + ''.join(set(text[0]))\n",
        "        except:\n",
        "            pass\n",
        "    uniques = uniques.join(set(row))\n",
        "    length = len(uniques)\n",
        "    indexes = list(range(length))\n",
        "\n",
        "    di = dict(zip(uniques, indexes))\n",
        "    return di , length\n",
        "\n",
        "\n",
        "def convert_to_int(dataset,dictionary):\n",
        "    length = 0\n",
        "\n",
        "    int_dataset = []\n",
        "    row = []\n",
        "    for text in dataset:\n",
        "        length += 1\n",
        "        row.clear()\n",
        "        try:\n",
        "            for char in text[0]:\n",
        "                number = dictionary[char]\n",
        "                row.append(number)\n",
        "        except:\n",
        "            pass\n",
        "        int_dataset.append(row)\n",
        "\n",
        "    return int_dataset\n",
        "\n",
        "dataset = [['ذَهَبَ مُحَمَّد اِلَى المدرسة صباحا'],\n",
        "               ['ذَهَبَ مُحَمَّد اِلَى المدرسة ليلا'],\n",
        "               ['عاد محمد من النادى']]\n",
        "_dictionary , length = get_dictonary(dataset)\n",
        "converted_dataset = convert_to_int(dataset, _dictionary)\n",
        "print(converted_dataset)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7, 9, 19, 10, 20, 14, 20, 19, 10, 20, 8, 10, 9, 1, 8, 9, 19, 12], [7, 9, 19, 10, 20, 14, 20, 19, 10, 20, 8, 10, 9, 1, 8, 9, 19, 12], [7, 9, 19, 10, 20, 14, 20, 19, 10, 20, 8, 10, 9, 1, 8, 9, 19, 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTgRWfS85krV",
        "colab_type": "text"
      },
      "source": [
        "# Model with embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5stDr2RP-uVD",
        "colab_type": "text"
      },
      "source": [
        "https://keras.io/layers/embeddings/\n",
        "\n",
        "embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRBlatVGOpvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "import numpy as np\n",
        "\n",
        "#import read_data\n",
        "\n",
        "def train_model(length):\n",
        "    _dropout_rate = 0.2\n",
        "    _dropout_rate_softmax = 0.5\n",
        "    _number_of_inputs = 300 #max number of characters per doc(tweet)\n",
        "    _batch_size = 10\n",
        "    _kernal_size= 5 #An integer or tuple/list of a single integer\n",
        "    _pool_size = 3\n",
        "    _noise_shape = (_batch_size,1,_number_of_inputs)\n",
        "    _epochs = 100\n",
        "    _test_size = 0.33 # percentage of test from the dataset\n",
        "    _Learning_rate = 0.0001\n",
        "    _input_dim_emb_layer = length # max value of a letter in htis vocab set\n",
        "    \n",
        "    _output_dim_emb_layer = 100   #Dimension of the dense embedding.\n",
        "    _vector_size = _output_dim_emb_layer #vector for each char\n",
        "    _input_length = 129 # max length in this dataset \n",
        "\n",
        "    X_train = np.load('C_X_train.npy')\n",
        "    X_test = np.load('C_X_test.npy')\n",
        "    y_train = np.load('C_y_train.npy')\n",
        "    y_test = np.load('C_y_test.npy')\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(y_train.shape)\n",
        "    print(X_test.shape)\n",
        "    print(y_test.shape)\n",
        "\n",
        "    #print(\"done input embedding\")\n",
        "    model = Sequential()\n",
        "\n",
        "    #input\n",
        "    #model.add(keras.layers.Input(shape=(_number_of_inputs,_vector_size)))\n",
        "\n",
        "    #Dropout\n",
        "    #model.add(keras.layers.Dropout(rate=_dropout_rate,input_shape=(_number_of_inputs,_vector_size)))#,noise_shape,random.randint(0,number_of_inputs)))\n",
        "\n",
        "    #Embedding\n",
        "    model.add(Embedding(_input_dim_emb_layer, _output_dim_emb_layer,input_length=_input_length))\n",
        "\n",
        "    #Convolution\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\",input_shape=(X_train.shape[0], X_train.shape[1], _output_dim_emb_layer)))\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "    #Convolution\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "    #Convolution\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "    #Convolution\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "    #Convolution\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.Conv1D(filters=_vector_size, kernel_size=_kernal_size, strides=1, padding='same', activation=\"relu\"))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = _pool_size, padding='same'))\n",
        "\n",
        "\n",
        "    #Dropout\n",
        "    model.add(keras.layers.Dropout(_dropout_rate_softmax))#,noise_shape,random.randint(0,number_of_inputs)))\n",
        "\n",
        "    #output\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
        "\n",
        "    opt = keras.optimizers.Adam(lr=_Learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False);\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "                             write_graph=True,\n",
        "                             write_grads=True,\n",
        "                             batch_size=_batch_size,\n",
        "                             write_images=True)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=_batch_size,\n",
        "        epochs=_epochs,\n",
        "        validation_data=(X_test, y_test),\n",
        "        shuffle=True,\n",
        "        callbacks=[tensorboard]\n",
        "    )\n",
        "    model.save(\"trail_1(embedded_layer).h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnmY_W7P5vPN",
        "colab_type": "text"
      },
      "source": [
        "# calling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYeEXxYF5x9X",
        "colab_type": "code",
        "outputId": "80ba1530-93ff-4b0f-8c85-4ae282315b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2553
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "_test_size = 0.33 # percentage of test from the dataset \n",
        "\n",
        "\n",
        "#read dataset\n",
        "dataset, one_hot_Y = read_dataset()\n",
        "_dictionary ,length= get_dictonary(dataset)\n",
        "\n",
        "#convert each character to integer number\n",
        "int_dataset = convert_to_int(dataset,_dictionary)\n",
        "print(np.array(int_dataset).shape)\n",
        "\n",
        "\n",
        "#split dataset \n",
        "C_X_train, C_X_test, C_y_train, C_y_test = train_test_split(int_dataset, one_hot_Y, test_size = _test_size, random_state=42)\n",
        "\n",
        "#save train_test data\n",
        "np.save('C_X_train.npy', C_X_train)\n",
        "np.save('C_X_test.npy', C_X_test)\n",
        "np.save('C_y_train.npy', C_y_train)\n",
        "np.save('C_y_test.npy', C_y_test)\n",
        "\n",
        "#train model\n",
        "train_model(length)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10065, 129)\n",
            "(6743, 129)\n",
            "(6743, 8)\n",
            "(3322, 129)\n",
            "(3322, 8)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 129, 100)          15500     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 129, 100)          50100     \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 129, 100)          50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 43, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 43, 100)           50100     \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 43, 100)           50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 15, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 15, 100)           50100     \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 15, 100)           50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 5, 100)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 5, 100)            50100     \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 5, 100)            50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 2, 100)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 2, 100)            50100     \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 2, 100)            50100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 1, 100)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 100)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 808       \n",
            "=================================================================\n",
            "Total params: 517,308\n",
            "Trainable params: 517,308\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 6743 samples, validate on 3322 samples\n",
            "Epoch 1/100\n",
            "6743/6743 [==============================] - 8s 1ms/step - loss: 2.0767 - acc: 0.1446 - val_loss: 2.0752 - val_acc: 0.1574\n",
            "Epoch 2/100\n",
            "6743/6743 [==============================] - 6s 876us/step - loss: 2.0728 - acc: 0.1532 - val_loss: 2.0733 - val_acc: 0.1574\n",
            "Epoch 3/100\n",
            "6743/6743 [==============================] - 6s 861us/step - loss: 2.0741 - acc: 0.1493 - val_loss: 2.0724 - val_acc: 0.1574\n",
            "Epoch 4/100\n",
            "6743/6743 [==============================] - 6s 916us/step - loss: 2.0739 - acc: 0.1547 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 5/100\n",
            "6743/6743 [==============================] - 6s 854us/step - loss: 2.0742 - acc: 0.1480 - val_loss: 2.0725 - val_acc: 0.1574\n",
            "Epoch 6/100\n",
            "6743/6743 [==============================] - 6s 861us/step - loss: 2.0738 - acc: 0.1495 - val_loss: 2.0717 - val_acc: 0.1574\n",
            "Epoch 7/100\n",
            "6743/6743 [==============================] - 7s 1ms/step - loss: 2.0736 - acc: 0.1496 - val_loss: 2.0716 - val_acc: 0.1574\n",
            "Epoch 8/100\n",
            "6743/6743 [==============================] - 6s 865us/step - loss: 2.0734 - acc: 0.1470 - val_loss: 2.0712 - val_acc: 0.1574\n",
            "Epoch 9/100\n",
            "6743/6743 [==============================] - 6s 867us/step - loss: 2.0731 - acc: 0.1473 - val_loss: 2.0710 - val_acc: 0.1574\n",
            "Epoch 10/100\n",
            "6743/6743 [==============================] - 6s 929us/step - loss: 2.0724 - acc: 0.1507 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 11/100\n",
            "6743/6743 [==============================] - 6s 856us/step - loss: 2.0729 - acc: 0.1575 - val_loss: 2.0711 - val_acc: 0.1574\n",
            "Epoch 12/100\n",
            "6743/6743 [==============================] - 6s 859us/step - loss: 2.0727 - acc: 0.1544 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 13/100\n",
            "6743/6743 [==============================] - 6s 859us/step - loss: 2.0726 - acc: 0.1563 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 14/100\n",
            "6743/6743 [==============================] - 6s 860us/step - loss: 2.0731 - acc: 0.1520 - val_loss: 2.0714 - val_acc: 0.1574\n",
            "Epoch 15/100\n",
            "6743/6743 [==============================] - 6s 861us/step - loss: 2.0725 - acc: 0.1499 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 16/100\n",
            "6743/6743 [==============================] - 6s 859us/step - loss: 2.0730 - acc: 0.1539 - val_loss: 2.0710 - val_acc: 0.1574\n",
            "Epoch 17/100\n",
            "6743/6743 [==============================] - 6s 869us/step - loss: 2.0726 - acc: 0.1520 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 18/100\n",
            "6743/6743 [==============================] - 6s 871us/step - loss: 2.0732 - acc: 0.1528 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 19/100\n",
            "6743/6743 [==============================] - 6s 867us/step - loss: 2.0728 - acc: 0.1501 - val_loss: 2.0710 - val_acc: 0.1574\n",
            "Epoch 20/100\n",
            "6743/6743 [==============================] - 7s 1ms/step - loss: 2.0726 - acc: 0.1510 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 21/100\n",
            "6743/6743 [==============================] - 6s 870us/step - loss: 2.0724 - acc: 0.1532 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 22/100\n",
            "6743/6743 [==============================] - 6s 870us/step - loss: 2.0719 - acc: 0.1562 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 23/100\n",
            "6743/6743 [==============================] - 6s 861us/step - loss: 2.0720 - acc: 0.1461 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 24/100\n",
            "6743/6743 [==============================] - 6s 865us/step - loss: 2.0725 - acc: 0.1507 - val_loss: 2.0706 - val_acc: 0.1574\n",
            "Epoch 25/100\n",
            "6743/6743 [==============================] - 6s 860us/step - loss: 2.0722 - acc: 0.1511 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 26/100\n",
            "6743/6743 [==============================] - 6s 862us/step - loss: 2.0722 - acc: 0.1514 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 27/100\n",
            "6743/6743 [==============================] - 6s 909us/step - loss: 2.0725 - acc: 0.1490 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 28/100\n",
            "6743/6743 [==============================] - 6s 861us/step - loss: 2.0720 - acc: 0.1523 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 29/100\n",
            "6743/6743 [==============================] - 6s 861us/step - loss: 2.0717 - acc: 0.1504 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 30/100\n",
            "6743/6743 [==============================] - 6s 927us/step - loss: 2.0729 - acc: 0.1520 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 31/100\n",
            "6743/6743 [==============================] - 6s 857us/step - loss: 2.0722 - acc: 0.1550 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 32/100\n",
            "6743/6743 [==============================] - 6s 860us/step - loss: 2.0722 - acc: 0.1530 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 33/100\n",
            "6743/6743 [==============================] - 7s 1ms/step - loss: 2.0722 - acc: 0.1541 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 34/100\n",
            "6743/6743 [==============================] - 6s 862us/step - loss: 2.0722 - acc: 0.1510 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 35/100\n",
            "6743/6743 [==============================] - 6s 859us/step - loss: 2.0725 - acc: 0.1536 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 36/100\n",
            "6743/6743 [==============================] - 6s 855us/step - loss: 2.0725 - acc: 0.1533 - val_loss: 2.0709 - val_acc: 0.1574\n",
            "Epoch 37/100\n",
            "6743/6743 [==============================] - 7s 986us/step - loss: 2.0723 - acc: 0.1505 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 38/100\n",
            "6743/6743 [==============================] - 6s 857us/step - loss: 2.0723 - acc: 0.1511 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 39/100\n",
            "6743/6743 [==============================] - 6s 875us/step - loss: 2.0720 - acc: 0.1485 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 40/100\n",
            "6743/6743 [==============================] - 6s 856us/step - loss: 2.0720 - acc: 0.1498 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 41/100\n",
            "6743/6743 [==============================] - 6s 860us/step - loss: 2.0721 - acc: 0.1504 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 42/100\n",
            "6743/6743 [==============================] - 6s 869us/step - loss: 2.0721 - acc: 0.1522 - val_loss: 2.0708 - val_acc: 0.1574\n",
            "Epoch 43/100\n",
            "6743/6743 [==============================] - 6s 870us/step - loss: 2.0716 - acc: 0.1513 - val_loss: 2.0707 - val_acc: 0.1574\n",
            "Epoch 44/100\n",
            "6743/6743 [==============================] - 6s 922us/step - loss: 2.0718 - acc: 0.1520 - val_loss: 2.0707 - val_acc: 0.1574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS4GJDHoQqoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "import numpy as np\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(50, 4))\n",
        "# the model will take as input an integer matrix of size (batch, input_length).\n",
        "# the largest integer (i.e. word index) in the input should be\n",
        "# no larger than 999 (vocabulary size).\n",
        "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
        "\n",
        "input_array = np.random.randint(50, size=(5, 5))\n",
        "print(input_array)\n",
        "\n",
        "model.compile('rmsprop', 'mse')\n",
        "output_array = model.predict(input_array)\n",
        "print(output_array.shape[2])\n",
        "#assert output_array.shape == (32, 10, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}